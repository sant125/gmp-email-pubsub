# Post LinkedIn - Observabilidade com Google Managed Prometheus

## Vers√£o 1: T√©cnica e Detalhada (post longo)

---

**Implementando Observabilidade em Kubernetes com Google Managed Prometheus (GMP)**

Recentemente implementei um sistema completo de alertas no GKE usando Google Managed Prometheus, e descobri v√°rias armadilhas que podem economizar horas de debug pra quem est√° come√ßando. üßµ

**O Problema**
Gerenciar Prometheus self-hosted em produ√ß√£o √© trabalhoso: precisa configurar storage persistente, pensar em escalabilidade, backup, alta disponibilidade. E se o Prometheus cai, voc√™ fica cego.

**A Solu√ß√£o: GMP**
O Google Managed Prometheus √© uma implementa√ß√£o gerenciada que roda no seu cluster, mas toda a complexidade (storage, scaling, HA) √© abstra√≠da. Voc√™ s√≥ se preocupa com o que importa: m√©tricas e alertas.

**Arquitetura do GMP**

O GMP √© composto por 4 componentes principais rodando no namespace `gmp-system`:

1Ô∏è‚É£ **Collector** (DaemonSet)
- Roda em cada node do cluster
- L√™ os CRDs PodMonitoring que voc√™ cria
- Faz scrape dos endpoints /metrics dos pods
- Envia dados pro backend gerenciado do GMP

2Ô∏è‚É£ **Rule Evaluator** (Deployment)
- L√™ os CRDs Rules que voc√™ cria
- Executa queries PromQL contra o backend
- Avalia se alertas devem disparar
- Envia alertas pro AlertManager

3Ô∏è‚É£ **AlertManager** (StatefulSet)
- Recebe alertas do Rule Evaluator
- Agrupa alertas similares
- Roteia para receivers (Email, Slack, PagerDuty)
- Implementa silences e repeat intervals

4Ô∏è‚É£ **GMP Operator** (Deployment)
- Gerencia o ciclo de vida dos CRDs
- Reconcilia estado desejado vs atual
- Configura os componentes acima

**Descoberta #1: Voc√™ N√ÉO precisa instrumentar sua app para alertas b√°sicos**

Essa foi uma revela√ß√£o importante. Existem 3 fontes de m√©tricas no K8s:

üìä **M√©tricas do kubelet (cAdvisor)**
- CPU, mem√≥ria, disco, rede
- Coletadas automaticamente via cgroups
- J√° dispon√≠veis no GMP sem fazer nada
- Exemplos: `container_memory_working_set_bytes`, `container_cpu_usage_seconds_total`

üìä **M√©tricas do kube-state-metrics**
- Estado dos recursos K8s (Pods, Deployments, Nodes)
- Tamb√©m coletadas automaticamente
- Exemplos: `kube_pod_status_phase`, `kube_deployment_status_replicas_available`

üìä **M√©tricas da aplica√ß√£o (custom)**
- Request rate, lat√™ncia, erros (Golden Signals)
- M√©tricas de neg√≥cio (vendas, filas, processamentos)
- Requer instrumenta√ß√£o (Prometheus client library)
- Precisa expor `/metrics` e criar `PodMonitoring` CRD

**Exemplo pr√°tico:**

Para alertar sobre mem√≥ria alta, voc√™ N√ÉO precisa instrumentar nada:

```yaml
apiVersion: monitoring.googleapis.com/v1
kind: Rules
metadata:
  name: infra-alerts
spec:
  groups:
  - name: resources
    rules:
    - alert: HighMemoryUsage
      expr: |
        (container_memory_working_set_bytes{container!=""}
        / container_spec_memory_limit_bytes{container!=""}) > 0.85
      for: 5m
```

Essa m√©trica j√° existe! O kubelet coleta automaticamente.

Mas para alertar sobre alta taxa de erro HTTP, voc√™ precisa instrumentar:

```yaml
- alert: HighErrorRate
  expr: |
    rate(http_requests_total{status=~"5.."}[5m])
    /
    rate(http_requests_total[5m]) > 0.01
  for: 2m
```

Essa m√©trica `http_requests_total` s√≥ existe se sua app expor.

**Descoberta #2: NUNCA use `or absent()` em alertas**

Esse foi um bug que me custou horas. A query que parecia correta:

```yaml
# ‚ùå ERRADO
expr: up{job="default/myapp"} == 0 or absent(up{job="default/myapp"})
```

O problema: o alerta dispara quando os pods caem, mas NUNCA resolve quando voltam!

Por qu√™? O `absent()` pode retornar `true` mesmo com pods ativos se houver s√©ries temporais antigas no TSDB.

**Solu√ß√£o correta:**

```yaml
# ‚úÖ CORRETO
expr: sum(up{job="default/myapp"}) < 1
```

Simples, confi√°vel, e resolve automaticamente quando os pods voltam.

**CRDs: A Interface com o GMP**

O GMP usa Custom Resource Definitions para configura√ß√£o:

**PodMonitoring**: Define o que coletar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: myapp-monitoring
  namespace: default
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics  # Nome da porta (n√£o n√∫mero!)
    interval: 30s
    path: /metrics
```

**Rules**: Define quando alertar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: Rules
metadata:
  name: myapp-alerts
  namespace: default
spec:
  groups:
  - name: app_alerts
    interval: 30s
    rules:
    - alert: PodDown
      expr: sum(up{job="default/myapp-monitoring"}) < 1
      for: 1m
      labels:
        severity: critical
```

**OperatorConfig**: Configura√ß√£o global (em gmp-public!)
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: OperatorConfig
metadata:
  namespace: gmp-public  # TEM que ser aqui
  name: config
spec:
  managedAlertmanager:
    enabled: true
    configSecret:
      name: alertmanager-config
```

**Timeline de um Alerta**

Do pod cair at√© o email chegar:

```
00:00  Pod morre ‚Üí up = 0
00:30  Collector faz scrape ‚Üí detecta up=0
01:00  Rule Evaluator executa query ‚Üí marca PENDING
02:00  Passou 1min (for: 1m) ‚Üí muda pra FIRING
02:10  AlertManager aguarda group_wait (10s)
02:10  Email enviado via SMTP
02:15  Email chega na caixa de entrada
```

Total: ~2-3 minutos. √â o esperado, n√£o √© bug!

**Key Takeaways**

‚úÖ GMP abstrai complexidade de storage, scaling e HA do Prometheus
‚úÖ M√©tricas de infra (CPU/mem) j√° existem via kubelet - n√£o precisa instrumentar
‚úÖ M√©tricas de app (request rate, lat√™ncia) precisam de instrumenta√ß√£o
‚úÖ Use `sum(up{...}) < 1` ao inv√©s de `or absent()` em alertas
‚úÖ CRDs s√£o a interface: PodMonitoring (coleta), Rules (alertas), OperatorConfig (config global)
‚úÖ AlertManager gerenciado incluso - s√≥ configurar SMTP no Secret

**Custos**

O GMP em si √© gr√°tis at√© 50GB/m√™s de m√©tricas (voc√™ dificilmente chega nisso em labs).

Voc√™ s√≥ paga o cluster GKE: ~$50-75/m√™s no GKE Autopilot.

**Reposit√≥rio**

Documentei tudo com exemplos pr√°ticos, troubleshooting e armadilhas comuns:
[link do seu repo]

Inclui:
- Arquitetura detalhada dos componentes
- Guia de m√©tricas: kubelet vs aplica√ß√£o
- Setup passo a passo (~15min)
- Debug e API do AlertManager

---

Qual foi sua maior dificuldade implementando observabilidade no K8s? üëá

#Kubernetes #Observability #Prometheus #GCP #SRE #DevOps #CloudNative

---

## Vers√£o 2: Mais Direta (post m√©dio) - COM ARQUITETURA E API

---

**Google Managed Prometheus: O que aprendi implementando alertas no GKE**

Implementei um sistema de observabilidade completo usando GMP e descobri 3 coisas que ningu√©m te conta:

**1Ô∏è‚É£ Voc√™ provavelmente n√£o precisa instrumentar sua app**

80% dos alertas de infra usam m√©tricas que j√° existem:
- CPU/Mem√≥ria ‚Üí kubelet (cAdvisor)
- Status de pods ‚Üí kube-state-metrics
- Deployments/ReplicaSets ‚Üí kube-state-metrics

Exemplo de alerta de mem√≥ria alta SEM instrumenta√ß√£o:
```yaml
expr: |
  (container_memory_working_set_bytes{container!=""}
  / container_spec_memory_limit_bytes{container!=""}) > 0.85
```

Voc√™ s√≥ precisa instrumentar (expor `/metrics`) para:
- Request rate / lat√™ncia
- M√©tricas de neg√≥cio (vendas, filas, etc)
- Golden Signals espec√≠ficos da app

**2Ô∏è‚É£ NUNCA use `or absent()` em queries de alerta**

Query que parece correta mas quebra tudo:
```yaml
# ‚ùå Alerta dispara mas NUNCA resolve
expr: up{job="myapp"} == 0 or absent(up{job="myapp"})
```

Por qu√™? O `absent()` pode retornar true mesmo com pods ativos.

Solu√ß√£o que funciona:
```yaml
# ‚úÖ Dispara E resolve
expr: sum(up{job="myapp"}) < 1
```

**3Ô∏è‚É£ A API do AlertManager √© essencial pra debug**

Quando o alerta n√£o chega, n√£o fica adivinhando. O AlertManager tem uma API REST que mostra tudo:

```bash
# Port-forward pro AlertManager
kubectl port-forward -n gmp-system svc/alertmanager 9093:9093

# Ver alertas ativos (FIRING, PENDING, RESOLVED)
curl http://localhost:9093/api/v2/alerts

# Ver config aplicada (SMTP, receivers, routes)
curl http://localhost:9093/api/v2/status
```

J√° me salvou v√°rias vezes:
- Email n√£o chegava? API mostrou que o alerta tava em "suppressed" (silenciado)
- Alerta n√£o disparava? API mostrou que a query n√£o matchava nada
- Config SMTP errada? API mostrou erro de autentica√ß√£o

**Arquitetura: Como tudo se conecta**

O GMP n√£o √© uma caixa preta. Tem 4 componentes rodando no seu cluster:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Seu Cluster GKE                ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ  Pod 1  ‚îÇ  ‚îÇ  Pod 2  ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ/metrics ‚îÇ  ‚îÇ/metrics ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  (gmp-system)   ‚îÇ
‚îÇ  ‚îÇ  1. Collector          ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  Scrape a cada 30s     ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ           ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  2. Rule Evaluator     ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  Executa PromQL        ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ           ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  3. AlertManager       ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  Agrupa/Roteia         ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Gmail SMTP   ‚îÇ
    ‚îÇ  ou SendGrid  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
          üìß Email
```

**O que cada componente faz:**

1. **Collector** (DaemonSet)
   - L√™ os CRDs `PodMonitoring` que voc√™ cria
   - Faz scrape dos endpoints /metrics
   - Envia dados pro backend gerenciado do GMP

2. **Rule Evaluator** (Deployment)
   - L√™ os CRDs `Rules` que voc√™ cria
   - Executa queries PromQL contra o backend
   - Marca alertas como PENDING ‚Üí FIRING ‚Üí RESOLVED

3. **AlertManager** (StatefulSet)
   - Recebe alertas do Rule Evaluator
   - Agrupa alertas similares (evita spam)
   - Envia pra receivers (Email, Slack, PagerDuty)
   - Exp√µe API REST na porta 9093 (debug!)

4. **Operator** (Deployment)
   - Gerencia o ciclo de vida dos CRDs
   - Reconcilia estado desejado vs atual

**Timeline: Do pod cair at√© o email chegar**

```
T+0s    Pod morre ‚Üí up = 0
T+30s   Collector detecta ‚Üí envia pro GMP
T+60s   Rule Evaluator: passou 1min (for: 1m) ‚Üí FIRING
T+70s   AlertManager: aguarda group_wait (10s) ‚Üí envia email
T+75s   üìß Email chega
```

Total: ~75 segundos. √â o esperado, n√£o √© bug!

**3 CRDs que voc√™ precisa conhecer:**

`PodMonitoring` ‚Üí O que coletar (selector, porta, intervalo)
`Rules` ‚Üí Quando alertar (PromQL, for, labels)
`OperatorConfig` ‚Üí Config global (SMTP, receivers) - TEM que estar em `gmp-public`!

**Por que GMP vs Prometheus self-hosted?**

‚úÖ Storage gerenciado (24 meses reten√ß√£o)
‚úÖ Scaling autom√°tico
‚úÖ Alta disponibilidade inclusa
‚úÖ AlertManager gerenciado com API
‚úÖ 50GB/m√™s gr√°tis

Voc√™ s√≥ paga o cluster GKE (~$50-75/m√™s).

**Vamos ver isso funcionando na pr√°tica?**

Simulei um problema real: escalar o app pra 0 r√©plicas (como se tivesse crashado).

```bash
# T+0s: Derrubo todos os pods
kubectl scale deployment sample-app --replicas=0

# Pods come√ßam a terminar...
```

**O que acontece nos bastidores:**

```
T+0s    kubectl scale --replicas=0
        ‚îî‚îÄ Pods come√ßam a terminar

T+30s   Collector faz scrape
        ‚îî‚îÄ Detecta up=0
        ‚îî‚îÄ Envia pro backend GMP

T+60s   Rule Evaluator executa query
        ‚îî‚îÄ sum(up{...}) < 1 ‚Üí true h√° 1min
        ‚îî‚îÄ Alerta muda: PENDING ‚Üí FIRING

T+70s   AlertManager processa
        ‚îî‚îÄ Aguarda group_wait (10s)
        ‚îî‚îÄ Agrupa alertas similares
        ‚îî‚îÄ Envia via SMTP

T+75s   üìß Email chega!
        Subject: [FIRING:1] PodDown default/sample-app
```

**Como validei?** Usando a API do AlertManager:

```bash
kubectl port-forward -n gmp-system svc/alertmanager 9093:9093
curl http://localhost:9093/api/v2/alerts | jq
```

Retorna o alerta ativo:
```json
{
  "labels": {
    "alertname": "PodDown",
    "severity": "critical"
  },
  "status": {
    "state": "active"
  },
  "startsAt": "2025-10-25T20:05:00Z"
}
```

[Imagem 1: Pods do GMP rodando (gmp-system)]
[Imagem 2: App escalado pra 0 - pods em Terminating]
[Imagem 3: JSON do alerta na API do AlertManager]
[Imagem 4: Email chegando com [FIRING:1] PodDown]

Total do fluxo: **~75 segundos** do problema at√© a notifica√ß√£o. R√°pido o suficiente pra reagir, mas n√£o t√£o r√°pido que gera falso positivo.

**Reposit√≥rio**

Documentei tudo com exemplos pr√°ticos, troubleshooting e API endpoints: [link do repo]

Inclui:
- Setup completo (~15min)
- Exemplos de queries que funcionam
- Como debugar com a API do AlertManager
- Comandos pra reproduzir essa demo
- Armadilhas comuns e solu√ß√µes

Qual sua experi√™ncia com Prometheus gerenciado? J√° usou a API do AlertManager pra debug? üëá

#Kubernetes #GCP #Observability #SRE #DevOps

---

## Vers√£o 3: Thread (v√°rios posts)

### Post 1: O Problema

---

**Por que gerenciar Prometheus self-hosted √© uma dor de cabe√ßa?**

Se voc√™ j√° rodou Prometheus em produ√ß√£o, sabe os problemas:

‚ùå Storage persistente (PV, escalabilidade, custos)
‚ùå Alta disponibilidade (m√∫ltiplas r√©plicas, thanos)
‚ùå Backup e disaster recovery
‚ùå Se o Prometheus cai, voc√™ fica cego
‚ùå Scaling manual quando o volume cresce

E o pior: voc√™ passa mais tempo gerenciando o sistema de observabilidade do que usando ele.

Testei o Google Managed Prometheus e mudou o jogo. üßµ

#Kubernetes #Observability #GCP

---

### Post 2: A Solu√ß√£o

---

**Google Managed Prometheus: Prometheus gerenciado que roda no seu cluster**

Como funciona:

O GMP instala 4 componentes no namespace `gmp-system`:

üîπ **Collector** (DaemonSet)
Faz scrape dos pods e envia pro backend gerenciado

üîπ **Rule Evaluator**
Executa queries PromQL e dispara alertas

üîπ **AlertManager**
Agrupa e roteia notifica√ß√µes (Email/Slack/PagerDuty)

üîπ **Operator**
Gerencia os CRDs e reconcilia configura√ß√µes

O backend (storage, query engine, HA) √© completamente gerenciado pelo Google.

Voc√™ s√≥ cria os YAMLs e pronto.

Custo: Gr√°tis at√© 50GB/m√™s. Voc√™ s√≥ paga o cluster GKE.

Pr√≥ximo post: A descoberta que mudou como penso sobre m√©tricas üëá

#Kubernetes #SRE #DevOps

---

### Post 3: A Descoberta

---

**Voc√™ N√ÉO precisa instrumentar sua app para alertas de infraestrutura**

Essa foi a descoberta mais importante.

Existem 3 tipos de m√©tricas no K8s:

**1. M√©tricas do kubelet (autom√°ticas)**
CPU, mem√≥ria, disco, rede
Coletadas via cgroups
‚úÖ J√° existem no GMP

**2. M√©tricas do kube-state-metrics (autom√°ticas)**
Status de pods, deployments, nodes
L√™ da API do K8s
‚úÖ J√° existem no GMP

**3. M√©tricas da aplica√ß√£o (manual)**
Request rate, lat√™ncia, m√©tricas de neg√≥cio
‚ùå Precisa instrumentar (Prometheus client)

Exemplo pr√°tico:

Alerta de mem√≥ria alta ‚Üí Usa kubelet (autom√°tico)
Alerta de taxa de erro ‚Üí Usa app (instrumenta√ß√£o)

Pr√≥ximo post: O erro que me custou horas de debug üëá

#Kubernetes #Observability #Prometheus

---

### Post 4: O Erro Cr√≠tico

---

**NUNCA use `or absent()` em queries de alerta no Prometheus**

Gastei horas debugando isso.

A query que parecia correta:

```yaml
expr: up{job="myapp"} == 0 or absent(up{job="myapp"})
```

O que aconteceu:
‚úÖ Pods caem ‚Üí alerta DISPARA
‚ùå Pods voltam ‚Üí alerta CONTINUA ATIVO

Por qu√™?
O `absent()` pode retornar true mesmo com pods ativos se houver s√©ries temporais antigas no TSDB.

**Solu√ß√£o:**

```yaml
expr: sum(up{job="myapp"}) < 1
```

Simples, confi√°vel, resolve automaticamente.

Key lesson: Queries PromQL precisam ser **revers√≠veis**. Se a condi√ß√£o para disparar √© X, a condi√ß√£o para resolver precisa ser ¬¨X.

Documentei isso e mais armadilhas no repo: [link]

Qual foi o bug mais dif√≠cil que voc√™ j√° encontrou em queries PromQL? üëá

#Prometheus #Observability #SRE #PromQL

---

## Dicas de Publica√ß√£o

**Melhor formato:**
- LinkedIn privilegia posts nativos (n√£o links externos no in√≠cio)
- Coloque o link do repo no primeiro coment√°rio ou no final
- Use quebras de linha para facilitar leitura
- Emojis com modera√ß√£o (apenas para destacar se√ß√µes)

**Hor√°rio:**
- Ter√ßa a quinta, 8h-10h ou 17h-19h (hor√°rio BR)
- Evite segunda de manh√£ e sexta √† tarde

**Engagement:**
- Responda todos os coment√°rios nas primeiras 2 horas
- Fa√ßa perguntas no final (call to action)
- Compartilhe em grupos relevantes (Kubernetes BR, SRE Brasil, etc)

**Hashtags:**
Use 3-5 hashtags relevantes:
- #Kubernetes (2.5M seguidores)
- #DevOps (3M seguidores)
- #SRE (relevante mas menor)
- #CloudNative (relevante)
- #GCP ou #GoogleCloud (se focado em GCP)

**Formato recomendado:**
Vers√£o 2 (post m√©dio) tem melhor custo-benef√≠cio:
- T√©cnico o suficiente pra agregar valor
- Curto o suficiente pra segurar aten√ß√£o
- Com c√≥digo pra demonstrar expertise
- Com call to action no final

Quer que eu ajuste algo ou crie uma vers√£o customizada?
