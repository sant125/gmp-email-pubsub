# Post LinkedIn - Google Managed Prometheus na PrÃ¡tica

---

**Google Managed Prometheus: Observabilidade no GKE sem gerenciar infraestrutura** ğŸš€

Acabei de implementar um sistema completo de alertas no GKE e quero compartilhar o que aprendi na prÃ¡tica sobre o Google Managed Prometheus (GMP).

## ğŸ¯ O que Ã© GMP?

GMP Ã© uma implementaÃ§Ã£o gerenciada do Prometheus que resolve o maior desafio da observabilidade: **nÃ£o precisar gerenciar a infraestrutura de monitoramento**.

A arquitetura Ã© genial:
- Os **componentes de coleta** rodam no seu cluster
- O **backend** (storage, query engine, HA) Ã© gerenciado pelo Google
- VocÃª sÃ³ se preocupa com mÃ©tricas e alertas

## ğŸ—ï¸ Arquitetura: 4 componentes principais

O GMP instala tudo no namespace `gmp-system`:

**1ï¸âƒ£ Collector (DaemonSet)**
- Roda em **cada node** do cluster
- LÃª os CRDs `PodMonitoring` que vocÃª cria
- Faz scrape dos endpoints `/metrics` dos seus pods
- Envia dados pro backend gerenciado (nÃ£o fica no cluster!)

**2ï¸âƒ£ Rule Evaluator (Deployment)**
- LÃª os CRDs `Rules` que vocÃª cria
- Executa queries PromQL contra o backend a cada intervalo (ex: 30s)
- Marca alertas: `PENDING` â†’ `FIRING` â†’ `RESOLVED`
- Manda alertas pro AlertManager

**3ï¸âƒ£ AlertManager (StatefulSet)**
- Recebe alertas do Rule Evaluator
- Agrupa alertas similares (evita spam)
- Implementa silences e repeat intervals
- Roteia pra receivers: Email, Slack, PagerDuty
- ExpÃµe **API REST** na porta 9093 (essencial pra debug!)

**4ï¸âƒ£ GMP Operator (Deployment)**
- Gerencia ciclo de vida dos CRDs
- Reconcilia estado desejado vs atual
- Orquestra os outros componentes

## ğŸ“ Na prÃ¡tica: 3 CRDs que vocÃª usa

**PodMonitoring** - Define O QUE coletar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: sample-app-monitoring
  namespace: default
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: metrics        # Nome da porta (nÃ£o nÃºmero!)
    interval: 30s        # Scrape a cada 30s
    path: /metrics
```

**Rules** - Define QUANDO alertar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: Rules
metadata:
  name: sample-app-alerts
  namespace: default
spec:
  groups:
  - name: availability
    interval: 30s
    rules:
    - alert: PodDown
      expr: sum(up{job="default/sample-app-monitoring"}) < 1
      for: 1m              # Aguarda 1min antes de disparar
      labels:
        severity: critical
      annotations:
        summary: "App estÃ¡ down!"
```

**OperatorConfig** - ConfiguraÃ§Ã£o global (SMTP, receivers)
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: OperatorConfig
metadata:
  namespace: gmp-public    # TEM que ser gmp-public!
  name: config
spec:
  managedAlertmanager:
    enabled: true
    configSecret:
      name: alertmanager-config    # Secret com config SMTP
```

## ğŸ§ª Testei na prÃ¡tica: Simulando um crash

Implementei um cenÃ¡rio real no lab: derrubar o app e ver o alerta chegar no email.

**Setup:**
- App: `sample-app` com 2 rÃ©plicas
- PodMonitoring configurado (scrape a cada 30s)
- Rule: alerta se `sum(up{...}) < 1` por mais de 1 minuto
- AlertManager: enviando email via Gmail SMTP

**Teste executado:**
```bash
# T+0s: Derrubo todas as rÃ©plicas
kubectl scale deployment sample-app --replicas=0

# Acompanho o que acontece...
kubectl get pods -w
```

## â±ï¸ Timeline completa (observado no lab):

```
T+0s    kubectl scale --replicas=0
        â””â”€ Pods comeÃ§am a terminar (Terminating)

T+30s   Collector faz prÃ³ximo scrape
        â””â”€ Detecta up=0 (targets down)
        â””â”€ Envia pro backend GMP

T+60s   Rule Evaluator executa query
        â””â”€ sum(up{...}) < 1 â†’ true
        â””â”€ Alerta estÃ¡ true hÃ¡ 1min (for: 1m)
        â””â”€ Muda estado: PENDING â†’ FIRING

T+70s   AlertManager recebe o alerta FIRING
        â””â”€ Aguarda group_wait (10s padrÃ£o)
        â””â”€ Agrupa alertas similares
        â””â”€ Envia via SMTP pro Gmail

T+75s   ğŸ“§ Email chega na caixa de entrada!
        Subject: [FIRING:1] PodDown default/sample-app
```

**Total: ~75 segundos** do crash atÃ© a notificaÃ§Ã£o.

## ğŸ” ValidaÃ§Ã£o: API do AlertManager

A API foi crucial pra entender o que estava acontecendo:

```bash
# Port-forward pro AlertManager
kubectl port-forward -n gmp-system svc/alertmanager 9093:9093

# Ver alertas ativos
curl http://localhost:9093/api/v2/alerts | jq
```

**Resposta (alerta FIRING):**
```json
[
  {
    "labels": {
      "alertname": "PodDown",
      "severity": "critical",
      "namespace": "default"
    },
    "status": {
      "state": "active",
      "silencedBy": [],
      "inhibitedBy": []
    },
    "startsAt": "2025-10-25T20:05:00Z",
    "endsAt": "0001-01-01T00:00:00Z",
    "fingerprint": "a1b2c3d4e5f6"
  }
]
```

Isso me mostrou:
âœ… O alerta estava ativo (`state: active`)
âœ… NÃ£o estava silenciado (`silencedBy: []`)
âœ… O timestamp exato do disparo

**Depois voltei os pods:**
```bash
kubectl scale deployment sample-app --replicas=2
```

~2 minutos depois, o alerta mudou pra `RESOLVED` e recebi email de resoluÃ§Ã£o!

## ğŸ’¡ Descobertas importantes

**1) VocÃª NÃƒO precisa instrumentar pra alertas de infra**

80% dos alertas usam mÃ©tricas que **jÃ¡ existem**:
- CPU/MemÃ³ria â†’ kubelet (cAdvisor)
- Status de pods â†’ kube-state-metrics
- A mÃ©trica `up` (target health) â†’ GMP

SÃ³ precisa instrumentar para:
- Request rate, latÃªncia, erros
- MÃ©tricas de negÃ³cio (vendas, filas)

**2) A API do AlertManager Ã© essencial**

Quando o alerta nÃ£o chega, use a API pra debugar:
- `/api/v2/alerts` â†’ Ver alertas (FIRING, PENDING, RESOLVED)
- `/api/v2/status` â†’ Ver config aplicada (SMTP, receivers)
- `/api/v2/silences` â†’ Ver silences ativos

**3) Backend gerenciado = menos preocupaÃ§Ã£o**

O que o Google gerencia:
âœ… Storage distribuÃ­do (24 meses retenÃ§Ã£o)
âœ… Query engine escalÃ¡vel
âœ… Alta disponibilidade
âœ… Backup automÃ¡tico

O que vocÃª gerencia:
ğŸ“ CRDs (PodMonitoring, Rules, OperatorConfig)
ğŸ“ Secret com SMTP

## ğŸ’° Custos observados

- GMP: **GrÃ¡tis** atÃ© 50GB/mÃªs
- No meu lab: usando ~2-3GB/mÃªs
- Custo real: cluster GKE Autopilot (~$50-75/mÃªs)

## ğŸ RepositÃ³rio com tudo documentado

Coloquei tudo no GitHub com:
- Setup completo passo a passo (~15min)
- Todos os YAMLs prontos pra usar
- Como configurar SMTP (Gmail + App Password)
- Troubleshooting e comandos de debug
- Arquitetura detalhada com diagramas
- Este mesmo teste que executei no lab

Link nos comentÃ¡rios! ğŸ‘‡

---

**O que vocÃª achou dessa abordagem de observabilidade gerenciada?**

JÃ¡ usou GMP, Amazon Managed Prometheus ou Grafana Cloud? Compartilha a experiÃªncia!

#Kubernetes #GCP #Observability #SRE #DevOps #Prometheus #CloudNative #GKE

---

## ğŸ“¸ Imagens sugeridas:

1ï¸âƒ£ Screenshot: 4 pods do GMP rodando em `gmp-system`
```bash
kubectl get pods -n gmp-system
```

2ï¸âƒ£ Screenshot: App escalado pra 0 rÃ©plicas
```bash
kubectl get pods  # Mostrando 0/0 ou Terminating
```

3ï¸âƒ£ Screenshot: JSON do alerta na API
```bash
curl localhost:9093/api/v2/alerts | jq
```

4ï¸âƒ£ Screenshot: Email recebido com `[FIRING:1] PodDown`

---

## ğŸ’¬ ComentÃ¡rio com link do repo:

```
ğŸ“š RepositÃ³rio completo com toda a implementaÃ§Ã£o:
[seu-link-github]

Inclui:
âœ… Setup passo a passo
âœ… YAMLs prontos pra copiar
âœ… Troubleshooting detalhado
âœ… Comandos de debug
âœ… Arquitetura explicada

Clone e teste vocÃª mesmo em 15 minutos! ğŸš€
```
