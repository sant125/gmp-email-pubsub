# Post LinkedIn - Google Managed Prometheus na Pr√°tica

---

**Google Managed Prometheus: Observabilidade no GKE sem gerenciar infraestrutura** üöÄ

Acabei de implementar um sistema completo de alertas no GKE e quero compartilhar o que aprendi na pr√°tica sobre o Google Managed Prometheus (GMP).

## üéØ O que √© GMP?

GMP √© uma implementa√ß√£o gerenciada do Prometheus que resolve o maior desafio da observabilidade: **n√£o precisar gerenciar a infraestrutura de monitoramento**.

A arquitetura √© genial:
- Os **componentes de coleta** rodam no seu cluster
- O **backend** (storage, query engine, HA) √© gerenciado pelo Google
- Voc√™ s√≥ se preocupa com m√©tricas e alertas

## üèóÔ∏è Arquitetura: 4 componentes principais

O GMP instala tudo no namespace `gmp-system`:

**1Ô∏è‚É£ Collector (DaemonSet)**
- Roda em **cada node** do cluster
- L√™ os CRDs `PodMonitoring` que voc√™ cria
- Faz scrape dos endpoints `/metrics` dos seus pods
- Envia dados pro backend gerenciado (n√£o fica no cluster!)

**2Ô∏è‚É£ Rule Evaluator (Deployment)**
- L√™ os CRDs `Rules` que voc√™ cria
- Executa queries PromQL contra o backend a cada intervalo (ex: 30s)
- Marca alertas: `PENDING` ‚Üí `FIRING` ‚Üí `RESOLVED`
- Manda alertas pro AlertManager

**3Ô∏è‚É£ AlertManager (StatefulSet)**
- Recebe alertas do Rule Evaluator
- Agrupa alertas similares (evita spam)
- Implementa silences e repeat intervals
- Roteia pra receivers: Email, Slack, PagerDuty
- Exp√µe **API REST** na porta 9093 (essencial pra debug!)

**4Ô∏è‚É£ GMP Operator (Deployment)**
- Gerencia ciclo de vida dos CRDs
- Reconcilia estado desejado vs atual
- Orquestra os outros componentes

## üìù Na pr√°tica: 3 CRDs que voc√™ usa

**PodMonitoring** - Define O QUE coletar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: sample-app-monitoring
  namespace: default
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: metrics        # Nome da porta (n√£o n√∫mero!)
    interval: 30s        # Scrape a cada 30s
    path: /metrics
```

**Rules** - Define QUANDO alertar
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: Rules
metadata:
  name: sample-app-alerts
  namespace: default
spec:
  groups:
  - name: availability
    interval: 30s
    rules:
    - alert: PodDown
      expr: kube_deployment_status_replicas_available{deployment="sample-app",namespace="default"} == 0
      for: 1m              # Aguarda 1min antes de disparar
      labels:
        severity: critical
      annotations:
        summary: "Deployment sem r√©plicas dispon√≠veis!"
        description: "Sample-app: {{ $value }} pods respondendo (esperado: >= 1)"
```

**OperatorConfig** - Configura√ß√£o global (SMTP, receivers)
```yaml
apiVersion: monitoring.googleapis.com/v1
kind: OperatorConfig
metadata:
  namespace: gmp-public    # TEM que ser gmp-public!
  name: config
spec:
  managedAlertmanager:
    enabled: true
    configSecret:
      name: alertmanager-config    # Secret com config SMTP
```

## üß™ Testei na pr√°tica: Simulando um crash

Implementei um cen√°rio real no lab: derrubar o app e ver o alerta chegar no email.

**Setup:**
- App: `sample-app` com 2 r√©plicas
- PodMonitoring configurado (scrape a cada 30s)
- Rule: alerta se `sum(up{...}) < 1` por mais de 1 minuto
- AlertManager: enviando email via Gmail SMTP

**Teste executado:**
```bash
# T+0s: Derrubo todas as r√©plicas
kubectl scale deployment sample-app --replicas=0

# Acompanho o que acontece...
kubectl get pods -w
```

## ‚è±Ô∏è Timeline completa (observado no lab):

```
T+0s    kubectl scale --replicas=0
        ‚îî‚îÄ Kubernetes atualiza deployment.status
        ‚îî‚îÄ Pods come√ßam a terminar (Terminating)

T+30s   Collector do kube-state-metrics faz scrape
        ‚îî‚îÄ Detecta replicas_available=0
        ‚îî‚îÄ Envia pro backend GMP

T+60s   Rule Evaluator executa query
        ‚îî‚îÄ kube_deployment_status_replicas_available == 0 ‚Üí true
        ‚îî‚îÄ Alerta est√° true h√° 1min (for: 1m)
        ‚îî‚îÄ Muda estado: PENDING ‚Üí FIRING

T+70s   AlertManager recebe o alerta FIRING
        ‚îî‚îÄ Aguarda group_wait (10s padr√£o)
        ‚îî‚îÄ Agrupa alertas similares
        ‚îî‚îÄ Envia via SMTP pro Gmail

T+75s   üìß Email chega na caixa de entrada!
        Subject: [FIRING:1] PodDown default/sample-app
```

**Total: ~75 segundos** do crash at√© a notifica√ß√£o.

## üîç Valida√ß√£o: API do AlertManager

A API foi crucial pra entender o que estava acontecendo:

```bash
# Port-forward pro AlertManager
kubectl port-forward -n gmp-system svc/alertmanager 9093:9093

# Ver alertas ativos
curl http://localhost:9093/api/v2/alerts | jq
```

**Resposta (alerta FIRING):**
```json
[
  {
    "labels": {
      "alertname": "PodDown",
      "severity": "critical",
      "namespace": "default"
    },
    "status": {
      "state": "active",
      "silencedBy": [],
      "inhibitedBy": []
    },
    "startsAt": "2025-10-25T20:05:00Z",
    "endsAt": "0001-01-01T00:00:00Z",
    "fingerprint": "a1b2c3d4e5f6"
  }
]
```

Isso me mostrou:
‚úÖ O alerta estava ativo (`state: active`)
‚úÖ N√£o estava silenciado (`silencedBy: []`)
‚úÖ O timestamp exato do disparo

**Depois voltei os pods:**
```bash
kubectl scale deployment sample-app --replicas=2
```

~2 minutos depois, o alerta mudou pra `RESOLVED` e recebi email de resolu√ß√£o!

## üí° Descobertas importantes

**1) Voc√™ N√ÉO precisa instrumentar pra alertas de infra**

80% dos alertas usam m√©tricas que **j√° existem**:
- CPU/Mem√≥ria ‚Üí kubelet (cAdvisor)
- Status de deployments/pods ‚Üí **kube-state-metrics** (a chave!)
- Network, disk I/O ‚Üí kubelet

S√≥ precisa instrumentar para:
- Request rate, lat√™ncia, erros (Golden Signals)
- M√©tricas de neg√≥cio (vendas, filas, convers√µes)

**2) Use kube-state-metrics, n√£o a m√©trica `up` para alertas de disponibilidade**

Essa foi a descoberta mais importante. Tentei v√°rias queries com a m√©trica `up`:

‚ùå `sum(up{job="..."}) < 1` ‚Üí N√£o dispara quando pods somem
‚ùå `up{job="..."} == 0 or absent(up{job="..."})` ‚Üí Dispara mas nunca resolve
‚ùå `count(up{job="..."} == 1) == 0` ‚Üí Resolve mas n√£o dispara

**Por qu√™?** No GMP com PodMonitoring, quando n√£o h√° pods, a m√©trica `up` **desaparece completamente** (no data). Agrega√ß√µes como `sum()` e `count()` retornam "no data" em vez de 0.

**Solu√ß√£o que funciona:**
```yaml
expr: kube_deployment_status_replicas_available{deployment="sample-app"} == 0
```

Usa **kube-state-metrics** que monitora o estado do Kubernetes diretamente. A m√©trica existe sempre, mesmo com 0 pods!

‚úÖ Dispara quando deployment tem 0 r√©plicas
‚úÖ Resolve quando deployment volta a ter r√©plicas
‚úÖ Funciona imediatamente (sem hist√≥rico)

**3) A API do AlertManager √© essencial**

Quando o alerta n√£o chega, use a API pra debugar:
- `/api/v2/alerts` ‚Üí Ver alertas (FIRING, PENDING, RESOLVED)
- `/api/v2/status` ‚Üí Ver config aplicada (SMTP, receivers)
- `/api/v2/silences` ‚Üí Ver silences ativos

**4) Backend gerenciado = menos preocupa√ß√£o**

O que o Google gerencia:
‚úÖ Storage distribu√≠do (24 meses reten√ß√£o)
‚úÖ Query engine escal√°vel
‚úÖ Alta disponibilidade
‚úÖ Backup autom√°tico

O que voc√™ gerencia:
üìù CRDs (PodMonitoring, Rules, OperatorConfig)
üìù Secret com SMTP

**Pr√©-requisito:** PodMonitoring para kube-state-metrics

Se usar m√©tricas do kube-state-metrics (como `kube_deployment_*`), voc√™ precisa criar um PodMonitoring para ele:

```yaml
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: kube-state-metrics
  namespace: gke-managed-cim    # Namespace do kube-state-metrics no GKE
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: gke-managed-kube-state-metrics
  endpoints:
  - port: k8s-objects
    interval: 30s
    path: /metrics
```

Depois de 1-2 minutos, as m√©tricas `kube_*` estar√£o dispon√≠veis no GMP!

## üí∞ Custos observados

- GMP: **Gr√°tis** at√© 50GB/m√™s
- No meu lab: usando ~2-3GB/m√™s
- Custo real: cluster GKE Autopilot (~$50-75/m√™s)

## üéÅ Reposit√≥rio com tudo documentado

Coloquei tudo no GitHub com:
- Setup completo passo a passo (~15min)
- Todos os YAMLs prontos pra usar
- Como configurar SMTP (Gmail + App Password)
- Troubleshooting e comandos de debug
- Arquitetura detalhada com diagramas
- Este mesmo teste que executei no lab

Link nos coment√°rios! üëá

---

**O que voc√™ achou dessa abordagem de observabilidade gerenciada?**

J√° usou GMP, Amazon Managed Prometheus ou Grafana Cloud? Compartilha a experi√™ncia!

#Kubernetes #GCP #Observability #SRE #DevOps #Prometheus #CloudNative #GKE

---

## üì∏ Imagens sugeridas:

### 1Ô∏è‚É£ Componentes do GMP rodando
```bash
kubectl get pods -n gmp-system
# Mostra: alertmanager-0, collector (DaemonSet), rule-evaluator, operator
```

### 2Ô∏è‚É£ kube-state-metrics e PodMonitoring
```bash
# Pod do kube-state-metrics
kubectl get pods -n gke-managed-cim

# PodMonitoring criado
kubectl get podmonitoring -n gke-managed-cim
```

### 3Ô∏è‚É£ Deployment escalado para 0
```bash
kubectl get deployment sample-app -n default
# Mostra: READY 0/0, AVAILABLE 0
```

### 4Ô∏è‚É£ Rules configurado com kube-state-metrics
```bash
kubectl get rules lab-alerts -n default -o yaml | grep -A 5 "expr:"
# Mostra: kube_deployment_status_replicas_available{...} == 0
```

### 5Ô∏è‚É£ Alerta ATIVO na API do AlertManager
```bash
kubectl port-forward -n gmp-system svc/alertmanager 9094:9093 &
curl -s http://localhost:9094/api/v2/alerts | jq '.[0] | {alert: .labels.alertname, status: .status.state, startsAt}'
```

### 6Ô∏è‚É£ Email recebido
- Screenshot do email com `[FIRING:1] PodDown`
- Screenshot do email com `[RESOLVED] PodDown` (quando pods voltam)

---

## üí¨ Coment√°rio com link do repo:

```
üìö Reposit√≥rio completo com toda a implementa√ß√£o:
[seu-link-github]

Inclui:
‚úÖ Setup passo a passo
‚úÖ YAMLs prontos pra copiar
‚úÖ Troubleshooting detalhado
‚úÖ Comandos de debug
‚úÖ Arquitetura explicada

Clone e teste voc√™ mesmo em 15 minutos! üöÄ
```
